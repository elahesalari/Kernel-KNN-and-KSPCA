# Kernel-KNN-and-KSPCA
Kernel k-nearest neighbor classifier and Kernel Principal Component Analysis

<b>Part 1:</b> You must to implement kernel k-nearest neighbor classifier. Recall that in 1NN 
classifier (ğ‘˜ = 1), we just need to compute the Euclidean distance of a test instance to all the training 
instances, find the closest training instance, and find its label. The label of the nest instance will be 
identical to its nearest neighbor. This can be kernelized by observing that:
<p align="center">
  <img 
    src="https://user-images.githubusercontent.com/91370511/159655405-f5170007-0419-4fc2-bd41-9e22028309d0.png"
  >
</p>


This allows us to apply the nearest neighbor classifier to structured data objects.
Implement the KNN classifier and kernel KNN classifier with Linear, RBF (tune the ğœ parameter with 
cross-validation), Polynomial (ğ‘‘ = 1), Polynomial (ğ‘‘ = 2), and Polynomial (ğ‘‘ = 3) kernels. Report the 
accuracy of classification for each data set with each classifier and compare the results. Split the data 
set into 70% and 30% for training and testing parts. You should report the mean of accuracies for 10 
individual runs. Report the running time of your code (in seconds) in the second table.

<b>Part 2:</b>
You should implement the Kernel Principal Component Analysis or KSPCA algorithm [1]. Implement the KSPCA algorithm according to the following pseudo-code.
<p align="center">
  <img 
    src="https://user-images.githubusercontent.com/91370511/159655927-99927a19-4790-49b5-bdf8-6416bb18728d.png"
  >
</p>

Consider data matrix ![image](https://user-images.githubusercontent.com/91370511/159656144-e08be6e1-5d99-4db8-b4de-e20f7f506aea.png) (ğ‘ is the dimensionality of the data in original space and ğ‘› is the number of training samples) and labels ğ‘Œ.
All you need is to compute delta kernel ğ¿, matrix ğ» (ğ» = ![image](https://user-images.githubusercontent.com/91370511/159656252-2d7db591-3981-4b68-9bf7-fe65ec8ba404.png)
in which ğ‘’ is a vector of all ones, ![image](https://user-images.githubusercontent.com/91370511/159664889-ecb2339b-8fb9-47f6-b3ed-8a126a8bf782.png), and kernel matrix ğ¾. ğ¼ is the identity matrix.
<br/>
â–ª ğ¿ is a ğ‘› Ã— ğ‘› delta kernel over ğ‘Œ (labels), compute ğ¿ with the following equation. 
<p align="center">
  <img 
    src="https://user-images.githubusercontent.com/91370511/159665394-34978afe-f394-4d75-9ef6-7443e847d507.png"
  >
</p>

For example, in a 2-class problem, if there are 5 data points such that the first 3 data points 
belong to class 1 and the fourth and the fifth data points are from class 2, ğ¿(ğ‘¦, ğ‘¦â€²) can be
formed as follows:
<p align="center">
  <img 
    src="https://user-images.githubusercontent.com/91370511/159665553-5820ce0a-0d3f-458f-bc9b-c9402d7e6972.png"
  >
</p>

â–ª ğ¾ is a ğ‘› Ã— ğ‘› RBF kernel over samples in ğ‘‹ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›, compute ğ¾ with the following equation.
<p align="center">
  <img 
    src="https://user-images.githubusercontent.com/91370511/159665676-f541212f-da0f-403b-9b49-e638212949cb.png"
  >
</p>

â–ª Split the data set into 70% and 30% for training and testing parts. ğ¾_ğ‘¡ğ‘’ğ‘ ğ‘¡ is a kernel over ğ‘‹_ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›
and ğ‘‹_ğ‘¡ğ‘’ğ‘ ğ‘¡. For example, if we have 70 samples in ğ‘‹_ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› and 30 samples in ğ‘‹_ğ‘¡ğ‘’ğ‘ ğ‘¡, ğ¾_ğ‘¡ğ‘’ğ‘ ğ‘¡ is a 
matrix of 70 Ã— 30, and ğ¾ is a matrix of 70 Ã— 70 over ğ‘‹_ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›. 
<br/>
â–ª ğ›½ can be computed via eigs command in Matlab or eigh in Python (scipy.linalg).
<br/>
â–ª Once you find the generalized eigenvectors of ğ‘„ and ğ¾, select the first 2 columns (ğ‘‘ = 2) and 
put them in ğ›½. Then you can find ğ‘§ and ğ‘ with matrix multiplication. ğ‘§ and ğ‘ are the test and 
train samples after projection. 
<br/>
â–ª Provide scatter plots of all the datasets(in KSPCA folder) in original space and after projection.
In other words, you should plot ğ‘‹ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› and ğ‘‹ğ‘¡ğ‘’ğ‘ ğ‘¡ in one plot and ğ‘§ and ğ‘ in another plot. Use 
different colors and symbols for different classes and different sets (train and test). You have 4 data sets, so you need to turn in 8 plots.
<br/>
â–ª For ğœ in RBF kernel, you should select it from the {0.1, 0.2, 0.3, â€¦ ,1} set. Select the ğœ which 
has the best separation between classes (tune it empirically). 

